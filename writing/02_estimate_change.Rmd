
## Datasets 

Datasets were retrieved from Metalab. As of February 2024, Metalab hosted 32 datasets in total, with research areas ranging from language learning to cognitive development. Each dataset synthesized the literature in one research area, with the scope of the dataset determined by the original contributor of the dataset. All datasets included effect size estimates converted to Cohen’s *d*, as well as estimates of effect size variance and a variety of other moderators (e.g., average age of participants) provided by the contributors. There were 2 desiderata for the datasets to be included in the final analysis: 

1. The dataset must describe an experimental (non-correlational) effect that uses behavioral measures, and 
2. For a dataset that has already been published, the aggregated meta-analytic effect reported in the published form must not be null (i.e., must be significantly different than zero). 

Five datasets did not meet the first desideratum (*Pointing and vocabulary (concurrent)*; *Pointing and vocabulary (longitudinal)*; *Video deficit*; *Symbolic play*; *Word segmentation (neuro)*), and one dataset did not meet the second desideratum (*Phonotactic learning*). These datasets were not included in the analysis. 

For the remaining 26 datasets, we made the following modifications to make their respective scope comparable such that each dataset corresponded to testing one distinct phenomenon. Following the organization in the original meta-analysis [@gasparini2021quantifying], we separated the *Language discrimination and preference* dataset into two datasets, one for discrimination and one for preference. We combined two pairs of datasets because they were testing the same experimental effects: *Gaze following (live)* and *Gaze following (video)* were combined into *Gaze following (combined)*; *Function word segmentation* and *Word segmentation (behavioral)* were combined into *Word segmentation (combined)*. We also replaced the *Infant directed speech preference* dataset with a more up-to-date version reported in @zettersten2024evidence. Finally, for one phenomenon that was predicted to follow a negative developmental trend (i.e. *Vowel discrimination (non-native*), we flipped the sign of the effect sizes to make the effects comparable with the rest of the phenomena.

Our goal is to estimate the functional form of developmental change in all of these meta-analytic datasets. To achieve this goal, we ran models with the same random effect structure specifications across all datasets. The random effect structure accounted for both experiment-level grouping and paper-level grouping. Not all datasets included these grouping variables initially so we recoded missing ones to make sure the same random effect structure specification could be applied to all datasets. 

```{r}
younger_than_36_df <- d %>% mutate(
 younger_than_36 = (mean_age_months <= 36) 
) %>% 
  group_by(younger_than_36) %>% 
  count() %>% 
  mutate(proportion = n / nrow(d))

propotion_younger <- round((filter(younger_than_36_df, younger_than_36 == TRUE) %>% 
pull(proportion)) * 100,2)

```

Since we were interested in the age trajectory of these constructs in early childhood, we further trimmed datasets to include only effect sizes from participant groups with an average age under 36 months. This decision did not qualitatively affect our findings as most datasets did not include data above age 36 months (`r propotion_younger`% of the effect sizes are from participants who were younger than 36 months of age). The final analysis included 25 datasets in total, each covering a different developmental time window. Table 1 shows these datasets, along with the number of effect sizes and participants included for each. 

## Methods

All statistical analyses were conducted in R. Meta-analytic models were fit using the `metafor` package [@viechtbauer2010conducting]. This was an exploratory study in which no hypotheses were pre-registered. All of the analysis scripts and data are available at [](https://github.com/anjiecao/metalabr_exp).

For each dataset, we considered four functional forms as possible candidates for the shape of the developmental trajectory: linear, logarithmic, quadratic, and constant. A linear form is the most common assumption in the literature, whereas logarithmic and quadratic were chosen to represent sublinear growth and superlinear growth, respectively. The constant form served as a baseline for the other alternative growth patterns. Although other, more complex growth patterns are of course possible, we opted to compare these forms as a first pass. Note that the constant model includes one parameter (an intercept), linear and logarithmic models include two parameters (an intercept and a slope), and the quadratic model includes three parameters (intercept, slope, and quadratic growth term). 

For all analyses, we fit multilevel random-effects meta-regression models using nested random intercepts to account for both the testing of the same infants in multiple conditions (e.g., in a between-participants design) and multiple studies within a single paper. Meta-regression models predicted effect sizes (Cohen’s $d$) with mean age in months in different functional forms. We fit four meta-regression models in total for each dataset. These four models collectively test the positive change assumption and the linearity assumption. If the positive change assumption is true, we should expect the other three models to outperform the constant model. If the linearity assumption is true, the linear model should be the best fitting model for that dataset. 

## Results




```{r}
delta_aic_df <- readRDS(here("cached_data/delta_aic_df.Rds"))
total_n <- nrow(delta_aic_df)
n_has_any_meaningful_difference <- delta_aic_df %>% 
  filter(if_any(where(is.numeric), ~ .x > 4)) %>% 
  nrow()
n_has_no_differnece <- total_n - n_has_any_meaningful_difference
```

### Model comparison 

Our initial goal was to compare the fit of models with different functional forms for each meta-analysis. Because models differed in their complexity (number of parameters), we extracted the corrected Akaike Information Criterion (AICc) for each model. AIC measures the quality of the model fits while penalizing models with more parameters. It is calculated as the difference between two times the number of estimated parameters in the model and two times the maximum value of the likelihood function of the model. The corrected AIC further adjusts for the number observations, which is particularly useful when the sample size is small relative to the parameters in the model. The model with the lowest AICc was considered the best fitting model, and all the remaining models were compared against it. The remaining model each received a $\Delta_{AIC}$, which was the difference between the AIC of the model and the AIC of the best fitting model. Following statistical convention, we treated $\Delta_{AIC} > 4$ as the statistical significance threshold [@burnham2004multimodel]. Models were considered significantly better than an alternative if the alternative model had $\Delta_{AIC} > 4$. Note that in the situation of a completely constant pattern of effects across age, the maximal difference in model fit would be an AICc of exactly 4 between the constant and quadratic model, because the former has one parameter and the latter has three parameters. 

Figure 1 shows model fits for each functional form. The four functional forms could not be meaningfully distinguished in `r n_has_no_differnece` out of `r total_n` datasets. This situation typically arises because the data are constant and hence more complex models with zero parameters fit the data equally well. The remaining `r n_has_any_meaningful_difference` datasets yielded meaningful contrasts between different functional forms, but the linear form was not the best-fitting form for any dataset. Table 2 shows the model comparison results for each dataset.

One limitation of the model comparison approach is that it does not quantify growth over time. To further examine the positive increase assumption, we estimated linear meta-regression models and examined the coefficient estimates for the linear age predictor. We found that the slope estimate for age was not significantly different from zero in the majority of datasets (16/25; Figure 2). 

## Discussion 

Across the 25 meta-analyses we examined, four functional forms for age-related change -- linear, logarithmic, quadratic, and constant -- were largely indistinguishable in most datasets. Notably, in none of the the 6 datasets where a model was significantly better than constant was the linear model the best fitting. This finding is inconsistent with the prevalent linearity assumption for early linguistic and cognitive development. 


Further, in direct statistical assessment of positive increases over age using regression models, we only detected evidence for linear growth in 9/25 meta-analyses. Past work has successfully revealed age-related changes using meta-analysis [e.g. @best2015age; @sugden2017meta; @mccartney1990growing]. But in most datasets that we have considered, effect size does not increase with age, despite theoretical reason to assume such a developmental change over age in the phenomena considered. Why? 


```{r fig.width=9, fig.height=10, fig.pos="!h", fig.cap="Each panel shows the dataset and the predicted values of the four functional forms. For each panel, X-axis represent the age in month, and Y-axis represents the effect size. The shaded area is the 95% confidence interval of the prediction."}
#fig1 <- readRDS(here("writing/display_items/figure_1.Rds"))
#fig1
```

```{r fig.cap="Each panel shows the dataset and the predicted values of the four functional forms. For each panel, X-axis represent the age in month, and Y-axis represents the effect size. The shaded area is the 95% confidence interval of the prediction.", fig.height=10, fig.pos="!h", fig.width=9, message=FALSE, warning=FALSE}
fig1 <- readRDS(here("writing/display_items/figure_1_freey.Rds"))
fig1
```

```{r fig.cap="Each panel shows the dataset and the predicted values of the four functional forms. For each panel,X-axis represent the age in month, and Y-axis represents the effect size. The shaded area is the 95% confidence interval of the prediction.", fig.height=10, fig.pos="!h", fig.width=9, message=FALSE, warning=FALSE}
#fig1 <- readRDS(here("writing/display_items/fig_1_freey_log.Rds"))
#fig1
```

```{r fig.cap="Each dot represents the estimate of the age predictor in the linear model. Red dotsindicate the particular estimate is statistically significant, and black indicate the estimate is not significant. Error bars show 95% confidence intervals.", fig.pos="!H", message=FALSE, warning=FALSE}
fig_2 <- readRDS(here("writing/display_items/figure_2.Rds"))
fig_2
```
