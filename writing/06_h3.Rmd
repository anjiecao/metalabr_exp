
## Theoretical constraints on effect sizes

```{r}
paper_info <- read_csv(here("data/rework/ma_paper_supplement.csv")) %>% 
  group_by(`With paper`) %>% 
  count()

better_half_info <- read_csv(here("data/rework/ma_paper_supplement.csv")) %>% 
  group_by(better_half_identified) %>% 
  count()

n_identify_better_half <- better_half_info %>% 
  filter(better_half_identified != "no", better_half_identified != "no (some by researcher variation)", !is.na(better_half_identified)) %>% 
  ungroup() %>% 
  count() %>% 
  pull(n)

n_has_paper <- paper_info %>% filter(`With paper` == "Yes") %>% pull(n)


```

Across the 25 datasets, `r n_has_paper` datasets were published through manuscripts in peer-reviewed journals. Among these manuscripts, we found that `r n_identify_better_half` papers reported that the meta-analytic effect was significantly stronger in a subset of the data. The subset was often identified by a particular condition in the experimental paradigm [e.g. experiment that shows “giving and taking action” to infants, @margoni2018infants], or certain characteristics of the participants [e.g. bilingual infants, @tsui2019associative]. In the rest of the data, the meta-analytic effect was either significantly weaker or not present at all. There are many reasons for why the effect would be stronger or only present in a subset of the data. Here, we remain agnostic to the underlying causes for these differences, and leverage these findings to ask: Is it possible that the influence of age was only observable in the subset of the dataset characterized by stronger effect sizes?  Perhaps noise in other conditions inadvertently masked age-related changes.

### Methods 

```{r}
model_comparison <- readRDS(here("cached_data/better_half_model_comparison.Rds"))
age_model_estimates <-  readRDS(here("cached_data/better_half_age_model_estimate.Rds"))%>% mutate_if(is.numeric, round, 2)
ma_estimates <- readRDS(here("cached_data/better_half_ma_estimate.Rds"))
wald_test_df <- readRDS(here("cached_data/better_half_wald_df.Rds")) %>% mutate_if(is.numeric, round, 2)

n_subsets_considered <- model_comparison %>% filter(ds_half != "full") %>% distinct(ds_half) %>% nrow()
```


We screened through `r n_has_paper` papers and identified `r n_identify_better_half` papers that reported a stronger effect on subsets of the data. All subsets had more than 10 effect sizes. For datasets reporting more than one subset as having strong effect, we consider each respectively. In sum, 7 datasets produced `r n_subsets_considered` subsets that showed stronger effects.  

We first investigated whether we could reproduce the original patterns, i.e. the effect sizes in the better halves were indeed stronger than the other halves. We ran the same multilevel meta-regression without any predictor to estimate the meta-analytic effect sizes in each half. Then we ran a Wald test to compare the two estimates by running a fixed-effects meta-regression model predicting effect sizes with the moderator distinguishing the two halves. A significant estimate on the moderator indicates that the meta-analytic effect sizes in both halves are significantly different from one another. We then estimated the slope of the age predictor in a multilevel meta-regression model for each of the subsets with larger effect sizes.

### Results and discussion 

```{r}

n_really_better <- wald_test_df %>% 
  filter(term == "metaworse") %>% 
  filter(p.value < 0.05) %>% 
  nrow()

psd_df <- wald_test_df %>% 
  filter(term == "metaworse") %>% 
  filter(half_name == "Prosocial agents_half1")

sscl_df <- wald_test_df %>% 
  filter(term == "metaworse") %>% 
  filter(half_name == "Statistical sound category learning_half1")

sws_df <- wald_test_df %>% 
  filter(term == "metaworse") %>% 
  filter(half_name == "Statistical word segmentation_half1")

```


We did not fully replicate the original findings reported in the original papers: the “better half” identified by the original meta-analysis did not produce significantly stronger effects than the rest of the data in many datasets. We did observe a significantly stronger effect in the remaining `r n_really_better` datasets: For *Prosocial Agents*, there was a stronger effect in experimental paradigms showing infants giving-taking actions compared to the studies showing infants other stimuli [@margoni2018infants, *z* = `r psd_df$statistic`, *p* = `r psd_df$p.value`]; For *Statistical Sound Category Learning*, stronger effect was observed in studies using habituation paradigm compared to other paradigms [@cristia2018can, *z* = `r sscl_df$statistic`, *p* = `r sscl_df$p.value`], and for *Statistical word segmentation*, stronger effect was observed in studies labeled as the conceptual replication of the original work [@black2017quantifying, *z* = `r sws_df$statistic`, *p* = `r sws_df$p.value`]. 

```{r}
me_df <- age_model_estimates %>% 
  filter(model_spec_clean == "Linear") %>% 
  filter(term == "mean_age_months") %>% 
  filter(ds_half == "Mutual exclusivity_half1")

scl_df <- age_model_estimates %>% 
  filter(model_spec_clean == "Linear") %>% 
  filter(term == "mean_age_months") %>% 
  filter(ds_half == "Statistical sound category learning_half1")

sb_df <- age_model_estimates %>% 
  filter(model_spec_clean == "Linear") %>% 
  filter(term == "mean_age_months") %>% 
  filter(ds_half == "Syntactic bootstrapping_half1")

st_df <- age_model_estimates %>% 
  filter(model_spec_clean == "Linear") %>% 
  filter(term == "mean_age_months") %>% 
  filter(ds_half == "Switch task_half2")

```


In addition, we did not find constraining our analyses to the “better half” increased the number of significant slope estimates. The two significant slope estimates came from *Mutual Exclusivity* ($\beta$ = `r me_df$estimate`, *SE* = `r me_df$std.error`, *z* = `r me_df$statistic`, *p* < 0.01) and *Statistical sound category learning* ($\beta$ = `r scl_df$estimate`, *SE* = `r scl_df$std.error`, *z* = `r scl_df$statistic`, *p* = `r scl_df$p.value`), which also showed significant slopes in the analyses with the full datasets. Qualitatively, we did see that the estimates increased in magnitude in *Syntactic bootstrapping* ($\beta$ = `r sb_df$estimate`, *SE* = `r sb_df$std.error`, *z* = `r sb_df$statistic`, *p* = `r sb_df$p.value`) and *Switch task* ($\beta$ = `r st_df$estimate`, *SE* = `r st_df$std.error`, *z* = `r st_df$statistic`, *p* = `r st_df$p.value`), but neither reached the statistical significance threshold. 

The discrepancy between our analyses and the previously reported finding suggested that the “better half effect” might not be sufficiently robust. This discrepancy could be attributed to the different statistical models we chose – in the original meta-analysis papers, the models tend to differ in their particular specification of the nested random effect structure and/or in the inclusions of moderators. We chose the simplest model with the maximum random effect structure per recommendation [@barr2013random]. This approach ensured fair comparison across all datasets, but it could diminish the strength of the reported effects. 

Interestingly, even in the datasets where the better half effect was reproduced, we failed to see a significant age effect in the same datasets (Prosocial agents and Statistical word segmentation) that did not show age-related changes in the original full dataset. Altogether, this set of analysis suggested that the theoretical constraints on the effect sizes could not adequately explain the lack of age-related change.

