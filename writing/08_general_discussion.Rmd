
# General discussion 

```{r}
all_slope_estimates <- readRDS(here("cached_data/all_slope_estimates.Rds"))

n_no_linear_sig <- all_slope_estimates %>% 
  filter(model_spec_clean == "Linear") %>% 
  filter(term == "mean_age_months") %>% 
  filter(p.value > .05) %>% 
  distinct(dataset) %>% 
  nrow()

n_any_sig <- all_slope_estimates %>% 
  filter(grepl( "mean_age_months", term)) %>% 
  filter(p.value > .05) %>% 
  distinct(dataset) %>% 
  nrow()

```


How do infantsâ€™ cognitive and linguistic abilities change with age? In this work, we leveraged a dataset of meta-analyses to evaluate the assumption that these abilities increase positively with age, and that the form of this increase is linear. There was no evidence for linear growth in `r n_no_linear_sig` datasets, and interestingly, in all of these datasets, there was no evidence for any age-related growth at all. In the second section, we investigated four potential explanations for this pattern:  (1) age-related selection bias against younger infants; (2) methodological adaptation for older infants; (3) change in only a subset of conditions; and (4) positive growth only after infancy.

Our current work has several limitations. First and foremost, we simply lacked sufficient data to investigate the possible explanations for many domains (see Table 3). In many datasets, when we filtered datasets to answer the corresponding questions, we lacked sufficient data to adequately test our hypotheses. Furthermore, as with many meta-analyses, our datasets also had high heterogeneity, meaning that we can only explain relatively small amounts of the variation among effect sizes (see Table 1). 

Our work highlights the importance of improving reporting standards in developmental psychology. Testing moderation of heterogeneity requires consistent coding of moderators across datasets. But surveys of reporting standards show that many potential moderators go unreported. For instance, fewer than half of papers report attrition rate [@raad2007brief; @nicholson2017attrition]. Given these observations, there is a clear need for the developmental psychology community to create and embrace more rigorous and transparent reporting standards. The recently developed framework for reporting demographics information across cultures in developmental psychology is one promising direction moving forwards [@singh2023unified]. Learning from other fields could provide valuable insights into how to enhance these standards. In biomedical research, numerous reporting standards have been published and widely adopted [for clinical trials: CONSORT, @schulz2010consort; for epidemiological research: STROBE, @vandenbroucke2007strengthening; for meta-analysis and systematic review, PRISMA: @moher2015preferred; for a catalog of reporting guidelines in health research: EQUATOR, @altman2008equator]. Following these structured guidelines in reporting could significantly increase both the quality and the quantity of information extractable from the original papers, providing more traction for tackling heterogeneity in meta-analysis.

Our work also underscores the importance of multi-laboratory large scale replication projects. The relationship between meta-analysis and multi-laboratory is complicated [@kvarven2020comparing; @lewis2022puzzling]. Although the latter approach is much more time- and resource- intensive than the former, it is also much more effective in controlling unwanted heterogeneity and detecting subtle patterns in the data. One prominent example is the comparison between the meta-analysis of Infant directed speech preference [@dunst2012preference] and the ManyBabies 1 project on the same topic [@manybabies2020quantifying]. @zettersten2023evidence found that, after an update to the meta-analysis dataset, both datasets yielded comparable estimated effect sizes (*d* = 0.35), but  the age effect was only detected in the MB1 project, not the meta-analysis. That study  speculated that our second explanation (methodological variation covarying with age) might account for their studies. In our analysis, we did investigate the methodological adaptation hypothesis in the IDS preference dataset. However, the methodological moderators available for us were limited and we could not incorporate the varying nature of the stimuli into our analysis. This example shows the potential limitations of meta-analyses that rely on aggregated data from studies with varied methodologies. In contrast, multi-laboratory collaboration projects like Manybabies [@visser2022improving] can rely on standardized data collection procedure and stimuli, therefore providing a more controlled dataset to answer a specific research question with high power. 

It is also worth considering whether the strengths of certain developmental phenomena truly stay constant throughout the first years of life. First of all, this counterintuitive possibility casts doubts on the construct validity of the existing measures. Many researchers strive to build on existing experimental procedures and measurements when they are testing older participants. This then leads to a potentially problematic situation: an experimental paradigm could have high construct validity with participants of a certain age, but low construct validity with participants of different age. As a result, this leads to an interesting conundrum: methodological adaptation could be a source of significant heterogeneity, diminishing the measurable developmental change. But at the same time, paradoxically, it could also be the prerequisite for properly measuring developmental change. Furthermore, an alternative explanation for the lack of developmental change is the limited sensitivity of cross-sectional design. The group average may stay constant, but there could still be growth in an individual's performance across development [@bornstein2017continuity]. The nuanced nature of developmental change might be best captured by dense, longitudinal data of individual child [e.g. @bergelson2023everyday; @sullivan2021saycam]. 

In sum, our current work presents a surprising finding concerning age-related change in the cognitive and language development literatures in early childhood.  Despite decades of research built upon the positive increase and linearity assumptions, we failed to find evidence supporting either in most meta-analyses that we had access to. Our work is not intended to overturn the longstanding developmental theories. Like other researchers, we believe that infants get better across different cognitive and linguistic domains as they get older. Instead, our work aims to highlight the needs for more robust reporting standards and more large-scale multi-laboratory projects that measure children consistently across age groups and over time. Our findings invite the cognitive development community to strengthen our understanding of foundational assumptions via collaborative efforts.

