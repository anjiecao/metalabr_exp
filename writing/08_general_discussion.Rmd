
# General Discussion 

```{r}
all_slope_estimates <- readRDS(here("cached_data/all_slope_estimates.Rds"))

n_no_linear_sig <- all_slope_estimates %>% 
  filter(model_spec_clean == "Linear") %>% 
  filter(term == "mean_age_months") %>% 
  filter(p.value > .05) %>% 
  distinct(dataset) %>% 
  nrow()

n_any_sig <- all_slope_estimates %>% 
  filter(grepl( "mean_age_months", term)) %>% 
  filter(p.value > .05) %>% 
  distinct(dataset) %>% 
  nrow()

```


How do infantsâ€™ cognitive and linguistic abilities change with age? In this work, we leveraged a dataset of meta-analyses to evaluate the assumption that these abilities increase positively with age, and that the form of this increase is linear. There was no evidence for linear growth in `r n_no_linear_sig` datasets, and interestingly, in all of these datasets, there was no evidence for any age-related growth at all. For the other nine datasets that did show developmental change, none of them had the linear model as the best-fitting model.

We investigated four potential explanations for this pattern:  (1) age-related selection bias against younger infants; (2) methodological adaptation for older infants; (3) change in only a subset of conditions; and (4) positive growth only after infancy. We showed that none of these hypotheses provide explanations for the lack of age-related growth in most datasets. Table 3 shows a summary of whether each hypothesis could explain the lack of linear growth in each dataset. Overall, we did not find that these hypotheses were consistent explanations of the lack of growth we observed.

<!-- Our current work has several strengths. By leveraging a large dataset of meta-analyses, we were able to conduct a comprehensive investigation of developmentals trend in cognitive and linguistic development across a wide range of domains. This broad, data-driven approach allows us to identify potential common patterns that might be missed in smaller studies. It also reveals that the linear form is not the best functional form to describe the developmental trajectories in datasets that showed a significant age related change. Furthermore, our investigation of the four hypotheses provides a thorough exploration of potential factors influencing the lack of developmental trend we observed. These analyses ensure that our conclusions are well-supported by the data.  -->

Our current work has several limitations that are intrinsic to the nature of our method. First and foremost, we simply lacked sufficient data to investigate the possible explanations for many domains (see Table 3). In many datasets, when we filtered datasets to answer the corresponding questions, we lacked sufficient data to adequately test our hypotheses. And as with many meta-analyses, our datasets also had high residual heterogeneity, meaning that we can only explain relatively small amounts of the variation among effect sizes, even when taking theoretically relevant factors into account.

Our work highlights the importance of improving reporting standards in developmental psychology. Testing moderation of heterogeneity requires consistent coding of moderators across datasets, but surveys of reporting standards show that many potential moderators go unreported. For instance, fewer than half of papers report attrition rate [@raad2007brief; @nicholson2017attrition]. Given these observations, there is a clear need for the developmental psychology community to create and embrace more rigorous and transparent reporting standards. Further, researchers could follow the open science practices and make their stimuli more publicly available. This would enable other researchers to conduct follow-up analyses such as investigating the visual complexity of stimuli. In addition, the recently developed framework for reporting demographics information across cultures in developmental psychology is also one promising direction moving forwards [@singh2023unified]. Learning from other fields could provide valuable insights into how to enhance these standards. In biomedical research, numerous reporting standards have been published and widely adopted [for clinical trials: CONSORT, @schulz2010consort; for epidemiological research: STROBE, @vandenbroucke2007strengthening; for meta-analysis and systematic review, PRISMA: @moher2015preferred; for a catalog of reporting guidelines in health research: EQUATOR, @altman2008equator]. Following these structured guidelines in reporting could significantly increase both the quality and the quantity of information extractable from the original papers, providing more traction for tackling heterogeneity in meta-analysis.

Our work also underscores the importance of multi-laboratory large scale replication projects. The relationship between meta-analysis and multi-laboratory is complicated [@kvarven2020comparing; @lewis2022puzzling]. Although the latter approach is much more time- and resource- intensive than the former, it is also much more effective in controlling unwanted heterogeneity and detecting subtle patterns in the data. One prominent example is the comparison between the meta-analysis of the infant directed speech preference [@dunst2012preference] and the ManyBabies 1 project on the same topic [@manybabies2020quantifying]. @zettersten2024evidence found that, after an update to the meta-analysis dataset, both datasets yielded comparable estimated effect sizes (*d* = 0.35), but that age-related change was only detected in the ManyBabies 1 project, not the meta-analysis. The study speculated that our second explanation (methodological variation covarying with age) might account for their results. In our analysis, we did investigate the methodological adaptation hypothesis in the IDS preference dataset. However, the methodological moderators available for us were limited and we could not incorporate the varying nature of the stimuli into our analysis. This example shows the potential limitations of meta-analyses that rely on aggregated data from studies with varied methodologies. In contrast, multi-laboratory collaboration projects like ManyBabies [@visser2022improving] can rely on standardized data collection procedure and stimuli, therefore providing a more controlled dataset to answer a specific research question with high power. 


It is worth considering whether the strengths of certain developmental phenomena truly stay constant throughout the first years of life. This counterintuitive possibility would cast doubts on the construct validity of existing measures. Many researchers strive to build on existing experimental procedures and measurements when they are testing participants of different ages. This optimization leads to a potentially problematic situation: an experimental paradigm could have high construct validity with participants of a certain age, but low construct validity with participants of different age [e.g. @rovee2009multiple]. This idea is consistent with our analysis showing no evidence for developmental change even within studies using the same methods as indicated by the coded methodological moderators. As a result, a conundrum emerges: methodological adaptation could be a source of significant heterogeneity, obscuring the measurable developmental change. But at the same time, paradoxically, it could also be the prerequisite for properly measuring developmental change. This dilemma calls attention to the importance of properly examining the psychometric properties of the measures used in cross-sectional developmental psychology research.

Last but not least, an alternative explanation for the lack of developmental change is the limited sensitivity of the cross-sectional design. The group average may stay constant, but there could still be growth in an individual's performance across development [@bornstein2017continuity]. The nuanced nature of developmental change might be best captured by dense, longitudinal data from individual children [e.g., @bergelson2023everyday; @sullivan2021saycam]. 

In sum, our current work presents a surprising finding concerning age-related change in the cognitive and language development literature in early childhood. Despite decades of research built upon the positive increase and linearity assumptions, we failed to find evidence supporting either in most meta-analyses that we had access to. Our work is not intended to overturn these longstanding developmental theories. Like other researchers, we believe that infants get better across different cognitive and linguistic domains as they get older. Instead, our work aims to highlight the need for more robust reporting standards and more large-scale multi-laboratory projects that measure children consistently across age groups and over time. Our findings invite the cognitive development community to strengthen our understanding of foundational assumptions via collaborative efforts.

